/home/rizar/Dist/theano/theano/gof/vm.py:719: UserWarning: CVM does not support memory profile, using Stack VM.
  'CVM does not support memory profile, using Stack VM.')
/home/rizar/Dist/theano/theano/scan_module/scan_perform_ext.py:117: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Function profiling
==================
  Message: ./demo.py:61
  Time in 5 calls to Function.__call__: 3.199580e+01s
  Time in Function.fn.__call__: 3.189055e+01s (99.671%)
  Time in thunks: 3.188097e+01s (99.641%)
  Total compile time: 3.089380e+00s
    Number of Apply nodes: 54
    Theano Optimizer time: 2.580852e+00s
       Theano validate time: 1.406312e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 4.979079e-01s

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  99.7%    99.7%      31.773s       3.18e+00s     Py      10       2   theano.scan_module.scan_op.Scan
   0.2%    99.9%       0.077s       3.07e-03s     C       25       5   theano.tensor.basic.Alloc
   0.1%   100.0%       0.030s       2.99e-03s     C       10       2   theano.tensor.subtensor.IncSubtensor
   0.0%   100.0%       0.000s       6.04e-06s     C       60      12   theano.tensor.elemwise.Elemwise
   0.0%   100.0%       0.000s       6.36e-06s     C       50      10   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.000s       5.08e-06s     C       35       7   theano.compile.ops.Shape_i
   0.0%   100.0%       0.000s       4.60e-06s     C       30       6   theano.tensor.elemwise.DimShuffle
   0.0%   100.0%       0.000s       5.28e-06s     C       15       3   theano.tensor.basic.Reshape
   0.0%   100.0%       0.000s       4.91e-06s     C       15       3   theano.compile.ops.Rebroadcast
   0.0%   100.0%       0.000s       3.41e-06s     C       20       4   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  72.5%    72.5%      23.117s       4.62e+00s     Py       5        1   forall_inplace,cpu,grad_of_layer_rec}
  27.2%    99.7%       8.657s       1.73e+00s     Py       5        1   for{cpu,layer_rec}
   0.2%    99.9%       0.077s       3.07e-03s     C       25        5   Alloc
   0.1%   100.0%       0.029s       5.85e-03s     C        5        1   IncSubtensor{InplaceInc;int64::}
   0.0%   100.0%       0.001s       1.27e-04s     C        5        1   IncSubtensor{InplaceInc;int64}
   0.0%   100.0%       0.000s       1.09e-05s     C       15        3   Elemwise{sub,no_inplace}
   0.0%   100.0%       0.000s       5.59e-06s     C       25        5   Subtensor{int64:int64:int64}
   0.0%   100.0%       0.000s       6.21e-06s     C       15        3   Subtensor{int64}
   0.0%   100.0%       0.000s       6.14e-06s     C       15        3   Shape_i{1}
   0.0%   100.0%       0.000s       4.29e-06s     C       20        4   Shape_i{0}
   0.0%   100.0%       0.000s       5.28e-06s     C       15        3   Reshape{3}
   0.0%   100.0%       0.000s       5.07e-06s     C       15        3   InplaceDimShuffle{1,0}
   0.0%   100.0%       0.000s       4.91e-06s     C       15        3   Rebroadcast{0}
   0.0%   100.0%       0.000s       3.41e-06s     C       20        4   ScalarFromTensor
   0.0%   100.0%       0.000s       1.15e-05s     C        5        1   Subtensor{::int64}
   0.0%   100.0%       0.000s       4.22e-06s     C       10        2   InplaceDimShuffle{0,1,x}
   0.0%   100.0%       0.000s       3.46e-06s     C       10        2   Elemwise{le,no_inplace}
   0.0%   100.0%       0.000s       5.53e-06s     C        5        1   Subtensor{int64:int64:int8}
   0.0%   100.0%       0.000s       5.39e-06s     C        5        1   Elemwise{Composite{[Switch(LT(*1 -> Composite{[Switch(LT(i0, i1), i1, i0)]}(Composite{[Switch(LT(i0, i1), i2, i0)]}(Composite{[sub(i0, Switch(LT(i1, i2), i2, i1))]}(i0, Composite{[add(i0, int_div(i1, i0))]}(i1, *2 -> add(i2, i0)), i3), i3, *2), i3), i4), *1, i4)]}}
   0.0%   100.0%       0.000s       5.01e-06s     C        5        1   Elemwise{Composite{[Switch(LT(i0, i1), i1, i0)]}}
   ... (remaining 6 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  72.5%    72.5%      23.117s       4.62e+00s      5    50                     forall_inplace,cpu,grad_of_layer_rec}(TensorConstant{50}, InplaceDimShuffle{0,2,1}.0, InplaceDimShuffle{0,1,x}.0, Elemwise{sub,no_inplace}.0, Subtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, Subtensor{::int64}.0, Rebroadcast{0}.0, Rebroadcast{0}.0, Rebroadcast{0}.0, G_rec, R_rec, W_rec, InplaceDimShuffle{1,0}.0, InplaceDimShuffle{1,0}.0, InplaceDimShuf
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float64, shape=(50, 1000, 80), strides=(-640000, 8, 8000) 
    input 2: dtype=float64, shape=(50, 80, 1), strides=(-640, 8, 8) 
    input 3: dtype=float64, shape=(50, 80, 1), strides=c 
    input 4: dtype=float64, shape=(50, 80, 1000), strides=(-640000, 8000, 8) 
    input 5: dtype=float64, shape=(50, 80, 1000), strides=(-640000, 8000, 8) 
    input 6: dtype=float64, shape=(50, 80, 1000), strides=(-640000, 8000, 8) 
    input 7: dtype=float64, shape=(50, 80, 1000), strides=(-640000, 8000, 8) 
    input 8: dtype=float64, shape=(51, 80, 1000), strides=(-640000, 8000, 8) 
    input 9: dtype=float64, shape=(1, 1000, 1000), strides=c 
    input 10: dtype=float64, shape=(1, 1000, 1000), strides=c 
    input 11: dtype=float64, shape=(1, 1000, 1000), strides=c 
    input 12: dtype=float64, shape=(1000, 1000), strides=c 
    input 13: dtype=float64, shape=(1000, 1000), strides=c 
    input 14: dtype=float64, shape=(1000, 1000), strides=c 
    input 15: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
    input 16: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
    input 17: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
    output 0: dtype=float64, shape=(51, 80, 1000), strides=(-640000, 8000, 8) 
    output 1: dtype=float64, shape=(1, 1000, 1000), strides=c 
    output 2: dtype=float64, shape=(1, 1000, 1000), strides=c 
    output 3: dtype=float64, shape=(1, 1000, 1000), strides=c 
  27.2%    99.7%       8.657s       1.73e+00s      5    47                     for{cpu,layer_rec}(TensorConstant{50}, InplaceDimShuffle{0,1,x}.0, Elemwise{sub,no_inplace}.0, Reshape{3}.0, Reshape{3}.0, Reshape{3}.0, TensorConstant{(51, 80, 1..00) of 0.0}, G_rec, R_rec, W_rec)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float64, shape=(50, 80, 1), strides=c 
    input 2: dtype=float64, shape=(50, 80, 1), strides=c 
    input 3: dtype=float64, shape=(50, 80, 1000), strides=c 
    input 4: dtype=float64, shape=(50, 80, 1000), strides=c 
    input 5: dtype=float64, shape=(50, 80, 1000), strides=c 
    input 6: dtype=float64, shape=(51, 80, 1000), strides=c 
    input 7: dtype=float64, shape=(1000, 1000), strides=c 
    input 8: dtype=float64, shape=(1000, 1000), strides=c 
    input 9: dtype=float64, shape=(1000, 1000), strides=c 
    output 0: dtype=float64, shape=(51, 80, 1000), strides=c 
   0.1%    99.8%       0.029s       5.85e-03s      5    10                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{51}, TensorConstant{80}, TensorConstant{1000})
    input 0: dtype=float64, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float64, shape=(51, 80, 1000), strides=c 
   0.1%    99.8%       0.029s       5.85e-03s      5    26                     IncSubtensor{InplaceInc;int64::}(Alloc.0, IncSubtensor{InplaceInc;int64}.0, Constant{1})
    input 0: dtype=float64, shape=(51, 80, 1000), strides=c 
    input 1: dtype=float64, shape=(50, 80, 1000), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(51, 80, 1000), strides=c 
   0.1%    99.9%       0.027s       5.41e-03s      5     9                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{50}, TensorConstant{80}, TensorConstant{1000})
    input 0: dtype=float64, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float64, shape=(50, 80, 1000), strides=c 
   0.0%   100.0%       0.007s       1.42e-03s      5    17                     Alloc(TensorConstant{0.0}, TensorConstant{1}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float64, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float64, shape=(1, 1000, 1000), strides=c 
   0.0%   100.0%       0.007s       1.33e-03s      5    16                     Alloc(TensorConstant{0.0}, TensorConstant{1}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float64, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float64, shape=(1, 1000, 1000), strides=c 
   0.0%   100.0%       0.007s       1.32e-03s      5    15                     Alloc(TensorConstant{0.0}, TensorConstant{1}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float64, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float64, shape=(1, 1000, 1000), strides=c 
   0.0%   100.0%       0.001s       1.27e-04s      5    18                     IncSubtensor{InplaceInc;int64}(Alloc.0, TensorConstant{(80, 1000) of 1.0}, Constant{-1})
    input 0: dtype=float64, shape=(50, 80, 1000), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(50, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.49e-05s      5    45                     Elemwise{sub,no_inplace}(TensorConstant{(1, 1, 1) of 1.0}, InplaceDimShuffle{0,1,x}.0)
    input 0: dtype=float64, shape=(1, 1, 1), strides=c 
    input 1: dtype=float64, shape=(50, 80, 1), strides=(-640, 8, 8) 
    output 0: dtype=float64, shape=(50, 80, 1), strides=c 
   0.0%   100.0%       0.000s       1.34e-05s      5    46                     Elemwise{sub,no_inplace}(TensorConstant{(1, 1, 1) of 1.0}, InplaceDimShuffle{0,1,x}.0)
    input 0: dtype=float64, shape=(1, 1, 1), strides=c 
    input 1: dtype=float64, shape=(50, 80, 1), strides=c 
    output 0: dtype=float64, shape=(50, 80, 1), strides=c 
   0.0%   100.0%       0.000s       1.15e-05s      5    29                     Subtensor{::int64}(IncSubtensor{InplaceInc;int64::}.0, Constant{-1})
    input 0: dtype=float64, shape=(51, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(51, 80, 1000), strides=(-640000, 8000, 8) 
   0.0%   100.0%       0.000s       1.04e-05s      5    51                     Subtensor{int64}(forall_inplace,cpu,grad_of_layer_rec}.2, Constant{0})
    input 0: dtype=float64, shape=(1, 1000, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(1000, 1000), strides=c 
   0.0%   100.0%       0.000s       9.06e-06s      5     0                     Shape_i{1}(W_rec)
    input 0: dtype=float64, shape=(1000, 1000), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.0%   100.0%       0.000s       8.68e-06s      5    48                     Subtensor{int64:int64:int64}(for{cpu,layer_rec}.0, Constant{49}, Constant{-52}, Constant{-1})
    input 0: dtype=float64, shape=(51, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(50, 80, 1000), strides=(-640000, 8000, 8) 
   0.0%   100.0%       0.000s       7.15e-06s      5    11                     Reshape{3}(ri, TensorConstant{[  50   80 1000]})
    input 0: dtype=float64, shape=(4000, 1000), strides=c 
    input 1: dtype=int64, shape=(3,), strides=c 
    output 0: dtype=float64, shape=(50, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       6.96e-06s      5     3                     InplaceDimShuffle{1,0}(G_rec)
    input 0: dtype=float64, shape=(1000, 1000), strides=c 
    output 0: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
   0.0%   100.0%       0.000s       5.77e-06s      5    19                     Subtensor{int64:int64:int64}(Reshape{3}.0, Constant{49}, Constant{-51}, Constant{-1})
    input 0: dtype=float64, shape=(50, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(50, 80, 1000), strides=(-640000, 8000, 8) 
   0.0%   100.0%       0.000s       5.53e-06s      5    42                     Subtensor{int64:int64:int8}(mask, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})
    input 0: dtype=float64, shape=(50, 80), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int8, shape=1, strides=c 
    output 0: dtype=float64, shape=(50, 80), strides=c 
   0.0%   100.0%       0.000s       5.44e-06s      5    41                     Subtensor{int64:int64:int64}(mask, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})
    input 0: dtype=float64, shape=(50, 80), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float64, shape=(50, 80), strides=(-640, 8) 
   ... (remaining 34 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 118500KB (118500KB)
    Max if linker=cvm(default): 87219KB (87250KB)
    Memory saved if views are used: 352313KB (352313KB)
    Memory saved if inplace ops are used: 118438KB (118438KB)
    Memory saved if gc is enabled: 31281KB (31250KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

      56640000B  [(51, 80, 1000), (1, 1000, 1000), (1, 1000, 1000), (1, 1000, 1000)] i i i i forall_inplace,cpu,grad_of_layer_rec}(TensorConstant{50}, InplaceDimShuffle{0,2,1}.0, InplaceDimShuffle{0,1,x}.0, Elemwise{sub,no_inplace}.0, Subtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, Subtensor{int64:int64:int64}.0, Subtensor{::int64}.0, Rebroadcast{0}.0, Rebroadcast{0}.0, Rebroadcast{0}.0, G_rec, R_rec, W_rec, InplaceDimShuffle{1,0}.0, InplaceDimShuffle{1,0}.0, InplaceDimShuffle{1,0}.0)
      32640000B  [(51, 80, 1000)] c for{cpu,layer_rec}(TensorConstant{50}, InplaceDimShuffle{0,1,x}.0, Elemwise{sub,no_inplace}.0, Reshape{3}.0, Reshape{3}.0, Reshape{3}.0, TensorConstant{(51, 80, 1..00) of 0.0}, G_rec, R_rec, W_rec)
      32640000B  [(51, 80, 1000)] v Subtensor{::int64}(IncSubtensor{InplaceInc;int64::}.0, Constant{-1})
      32640000B  [(51, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{51}, TensorConstant{80}, TensorConstant{1000})
      32640000B  [(51, 80, 1000)] i IncSubtensor{InplaceInc;int64::}(Alloc.0, IncSubtensor{InplaceInc;int64}.0, Constant{1})
      32000000B  [(50, 1000, 80)] v InplaceDimShuffle{0,2,1}(Subtensor{int64:int64:int64}.0)
      32000000B  [(50, 80, 1000)] v Reshape{3}(zi, TensorConstant{[  50   80 1000]})
      32000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(Reshape{3}.0, Constant{49}, Constant{-51}, Constant{-1})
      32000000B  [(50, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{50}, TensorConstant{80}, TensorConstant{1000})
      32000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(for{cpu,layer_rec}.0, Constant{49}, Constant{-52}, Constant{-1})
      32000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(Reshape{3}.0, Constant{49}, Constant{-51}, Constant{-1})
      32000000B  [(50, 80, 1000)] v Reshape{3}(ri, TensorConstant{[  50   80 1000]})
      32000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(Reshape{3}.0, Constant{49}, Constant{-51}, Constant{-1})
      32000000B  [(50, 80, 1000)] v Reshape{3}(x, TensorConstant{[  50   80 1000]})
      32000000B  [(50, 80, 1000)] i IncSubtensor{InplaceInc;int64}(Alloc.0, TensorConstant{(80, 1000) of 1.0}, Constant{-1})
       8000000B  [(1000, 1000)] v Subtensor{int64}(forall_inplace,cpu,grad_of_layer_rec}.3, Constant{0})
       8000000B  [(1, 1000, 1000)] v Rebroadcast{0}(Alloc.0)
       8000000B  [(1, 1000, 1000)] v Rebroadcast{0}(Alloc.0)
       8000000B  [(1000, 1000)] v InplaceDimShuffle{1,0}(G_rec)
       8000000B  [(1, 1000, 1000)] v Rebroadcast{0}(Alloc.0)
   ... (remaining 34 Apply account for 56192154B/603392154B ((9.31%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( layer_rec )
==================
  Message: None
  Time in 5 calls of the op (for a total of 250 steps) 8.656138e+00s

  Total time spent in calling the VM 8.580062e+00s (99.121%)
  Total overhead (computing slices..) 7.607532e-02s (0.879%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  75.7%    75.7%       6.437s       8.58e-03s     C      750       3   theano.tensor.blas.Gemm
  24.3%   100.0%       2.064s       4.13e-03s     C      500       2   theano.tensor.elemwise.Elemwise
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  75.7%    75.7%       6.437s       8.58e-03s     C      750        3   Gemm{no_inplace}
  17.3%    93.0%       1.469s       5.88e-03s     C      250        1   Elemwise{Composite{[add(mul(i0, add(mul(*1 -> scalar_sigmoid(i1), tanh(i2)), mul(sub(i3, *1), i4))), mul(i5, i4))]}}[(0, 1)]
   7.0%   100.0%       0.594s       2.38e-03s     C      250        1   Elemwise{Composite{[mul(scalar_sigmoid(i0), i1)]}}[(0, 0)]
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  25.4%    25.4%       2.160s       8.64e-03s    250     0                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, R_rec_copy, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
  25.2%    50.6%       2.139s       8.55e-03s    250     3                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, Elemwise{Composite{[mul(scalar_sigmoid(i0), i1)]}}[(0, 0)].0, W_rec_copy, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
  25.2%    75.7%       2.138s       8.55e-03s    250     1                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, G_rec_copy, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
  17.3%    93.0%       1.469s       5.88e-03s    250     4                     Elemwise{Composite{[add(mul(i0, add(mul(*1 -> scalar_sigmoid(i1), tanh(i2)), mul(sub(i3, *1), i4))), mul(i5, i4))]}}[(0, 1)](<TensorType(float64, col)>, Gemm{no_inplace}.0, Gemm{no_inplace}.0, TensorConstant{(1, 1) of 1.0}, <TensorType(float64, matrix)>, <TensorType(float64, col)>)
    input 0: dtype=float64, shape=(80, 1), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1, 1), strides=c 
    input 4: dtype=float64, shape=(80, 1000), strides=c 
    input 5: dtype=float64, shape=(80, 1), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   7.0%   100.0%       0.594s       2.38e-03s    250     2                     Elemwise{Composite{[mul(scalar_sigmoid(i0), i1)]}}[(0, 0)](Gemm{no_inplace}.0, <TensorType(float64, matrix)>)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 1875KB (1875KB)
    Max if linker=cvm(default): 1250KB (1875KB)
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 1250KB (1250KB)
    Memory saved if gc is enabled: 625KB (0KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

        640000B  [(80, 1000)] i Elemwise{Composite{[add(mul(i0, add(mul(*1 -> scalar_sigmoid(i1), tanh(i2)), mul(sub(i3, *1), i4))), mul(i5, i4))]}}[(0, 1)](<TensorType(float64, col)>, Gemm{no_inplace}.0, Gemm{no_inplace}.0, TensorConstant{(1, 1) of 1.0}, <TensorType(float64, matrix)>, <TensorType(float64, col)>)
        640000B  [(80, 1000)] i Elemwise{Composite{[mul(scalar_sigmoid(i0), i1)]}}[(0, 0)](Gemm{no_inplace}.0, <TensorType(float64, matrix)>)
        640000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, R_rec_copy, TensorConstant{1.0})
        640000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, Elemwise{Composite{[mul(scalar_sigmoid(i0), i1)]}}[(0, 0)].0, W_rec_copy, TensorConstant{1.0})
        640000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, G_rec_copy, TensorConstant{1.0})
   ... (remaining 0 Apply account for    0B/3200000B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( grad_of_layer_rec )
==================
  Message: None
  Time in 5 calls of the op (for a total of 250 steps) 2.311603e+01s

  Total time spent in calling the VM 2.231934e+01s (96.554%)
  Total overhead (computing slices..) 7.966907e-01s (3.446%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  70.4%    70.4%      15.506s       8.86e-03s     C     1750       7   theano.tensor.blas.Gemm
  19.8%    90.2%       4.359s       8.72e-03s     C      500       2   theano.tensor.blas.Dot22
   9.8%   100.0%       2.158s       9.59e-04s     C     2250       9   theano.tensor.elemwise.Elemwise
   0.0%   100.0%       0.001s       5.48e-06s     C      250       1   theano.tensor.elemwise.DimShuffle
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  60.6%    60.6%      13.341s       8.89e-03s     C     1500        6   Gemm{no_inplace}
  19.8%    80.4%       4.359s       8.72e-03s     C      500        2   Dot22
   9.8%    90.2%       2.165s       8.66e-03s     C      250        1   Gemm{inplace}
   5.4%    95.6%       1.198s       2.40e-03s     C      500        2   Elemwise{ScalarSigmoid}[(0, 0)]
   2.9%    98.6%       0.645s       2.58e-03s     C      250        1   Elemwise{Tanh}[(0, 0)]
   0.4%    99.0%       0.086s       3.43e-04s     C      250        1   Elemwise{Composite{[add(mul(i0, i1), mul(i2, i3, i4), mul(i3, i5), i6, i7)]}}[(0, 0)]
   0.3%    99.3%       0.074s       2.97e-04s     C      250        1   Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}
   0.2%    99.5%       0.053s       2.11e-04s     C      250        1   Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)]
   0.2%    99.7%       0.044s       1.77e-04s     C      250        1   Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}
   0.1%    99.9%       0.031s       1.26e-04s     C      250        1   Elemwise{mul,no_inplace}
   0.1%   100.0%       0.027s       1.07e-04s     C      250        1   Elemwise{sub}
   0.0%   100.0%       0.001s       5.48e-06s     C      250        1   InplaceDimShuffle{1,0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  10.4%    10.4%       2.295s       9.18e-03s    250    16                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}.0, TensorConstant{1.0})
    input 0: dtype=float64, shape=(1000, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(1000, 80), strides=(8, 8000) 
    input 3: dtype=float64, shape=(80, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(1000, 1000), strides=c 
  10.4%    20.8%       2.292s       9.17e-03s    250    11                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}.0, TensorConstant{1.0})
    input 0: dtype=float64, shape=(1000, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(1000, 80), strides=(8, 8000) 
    input 3: dtype=float64, shape=(80, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(1000, 1000), strides=c 
  10.2%    31.0%       2.251s       9.00e-03s    250    13                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)].0, TensorConstant{1.0})
    input 0: dtype=float64, shape=(1000, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(1000, 80), strides=(8, 8000) 
    input 3: dtype=float64, shape=(80, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(1000, 1000), strides=c 
   9.9%    41.0%       2.184s       8.73e-03s    250    12                     Dot22(Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}.0, G_rec_copy.T_replace)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   9.9%    50.8%       2.178s       8.71e-03s    250     0                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, G_rec_copy, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   9.9%    60.7%       2.175s       8.70e-03s    250    14                     Dot22(Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)].0, W_rec_copy.T_replace)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   9.8%    70.6%       2.166s       8.66e-03s    250     1                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, R_rec_copy, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   9.8%    80.4%       2.165s       8.66e-03s    250    17                     Gemm{inplace}(Dot22.0, TensorConstant{1.0}, Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}.0, R_rec_copy.T_replace, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=(8, 8000) 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   9.8%    90.2%       2.159s       8.64e-03s    250     6                     Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, Elemwise{mul,no_inplace}.0, W_rec_copy, TensorConstant{1.0})
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(1000, 1000), strides=c 
    input 4: dtype=float64, shape=(), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   2.9%    93.1%       0.645s       2.58e-03s    250     8                     Elemwise{Tanh}[(0, 0)](Gemm{no_inplace}.0)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   2.7%    95.9%       0.601s       2.40e-03s    250     2                     Elemwise{ScalarSigmoid}[(0, 0)](Gemm{no_inplace}.0)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   2.7%    98.6%       0.597s       2.39e-03s    250     3                     Elemwise{ScalarSigmoid}[(0, 0)](Gemm{no_inplace}.0)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.4%    99.0%       0.086s       3.43e-04s    250    18                     Elemwise{Composite{[add(mul(i0, i1), mul(i2, i3, i4), mul(i3, i5), i6, i7)]}}[(0, 0)](Dot22.0, Elemwise{ScalarSigmoid}[(0, 0)].0, Elemwise{sub}.0, <TensorType(float64, matrix)>, <TensorType(float64, col)>, <TensorType(float64, col)>, <TensorType(float64, matrix)>, Gemm{inplace}.0)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(80, 1000), strides=c 
    input 4: dtype=float64, shape=(80, 1), strides=c 
    input 5: dtype=float64, shape=(80, 1), strides=c 
    input 6: dtype=float64, shape=(80, 1000), strides=c 
    input 7: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.3%    99.3%       0.074s       2.97e-04s    250     9                     Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}(Elemwise{sub}.0, Elemwise{Tanh}[(0, 0)].0, <TensorType(float64, matrix)>, <TensorType(float64, col)>, <TensorType(float64, matrix)>, Elemwise{ScalarSigmoid}[(0, 0)].0)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(80, 1), strides=c 
    input 4: dtype=float64, shape=(80, 1000), strides=c 
    input 5: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.2%    99.5%       0.053s       2.11e-04s    250    10                     Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)](TensorConstant{(1, 1) of 1.0}, Elemwise{Tanh}[(0, 0)].0, Elemwise{ScalarSigmoid}[(0, 0)].0, <TensorType(float64, matrix)>, <TensorType(float64, col)>)
    input 0: dtype=float64, shape=(1, 1), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(80, 1000), strides=c 
    input 4: dtype=float64, shape=(80, 1), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.2%    99.7%       0.044s       1.77e-04s    250    15                     Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}(TensorConstant{(1, 1) of 1.0}, Elemwise{ScalarSigmoid}[(0, 0)].0, Dot22.0, <TensorType(float64, matrix)>)
    input 0: dtype=float64, shape=(1, 1), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    input 2: dtype=float64, shape=(80, 1000), strides=c 
    input 3: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.1%    99.9%       0.031s       1.26e-04s    250     5                     Elemwise{mul,no_inplace}(Elemwise{ScalarSigmoid}[(0, 0)].0, <TensorType(float64, matrix)>)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.1%   100.0%       0.027s       1.07e-04s    250     4                     Elemwise{sub}(TensorConstant{(1, 1) of 1.0}, Elemwise{ScalarSigmoid}[(0, 0)].0)
    input 0: dtype=float64, shape=(1, 1), strides=c 
    input 1: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(80, 1000), strides=c 
   0.0%   100.0%       0.001s       5.48e-06s    250     7                     InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0)
    input 0: dtype=float64, shape=(80, 1000), strides=c 
    output 0: dtype=float64, shape=(1000, 80), strides=(8, 8000) 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 29063KB (29063KB)
    Max if linker=cvm(default): 27188KB (26563KB)
    Memory saved if views are used: 625KB (625KB)
    Memory saved if inplace ops are used: 3750KB (3750KB)
    Memory saved if gc is enabled: 1875KB (2500KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

       8000000B  [(1000, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}.0, TensorConstant{1.0})
       8000000B  [(1000, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)].0, TensorConstant{1.0})
       8000000B  [(1000, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}.0, TensorConstant{1.0})
        640000B  [(80, 1000)] i Elemwise{Composite{[add(mul(i0, i1), mul(i2, i3, i4), mul(i3, i5), i6, i7)]}}[(0, 0)](Dot22.0, Elemwise{ScalarSigmoid}[(0, 0)].0, Elemwise{sub}.0, <TensorType(float64, matrix)>, <TensorType(float64, col)>, <TensorType(float64, col)>, <TensorType(float64, matrix)>, Gemm{inplace}.0)
        640000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, R_rec_copy, TensorConstant{1.0})
        640000B  [(80, 1000)] i Elemwise{ScalarSigmoid}[(0, 0)](Gemm{no_inplace}.0)
        640000B  [(80, 1000)] i Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)](TensorConstant{(1, 1) of 1.0}, Elemwise{Tanh}[(0, 0)].0, Elemwise{ScalarSigmoid}[(0, 0)].0, <TensorType(float64, matrix)>, <TensorType(float64, col)>)
        640000B  [(80, 1000)] c Dot22(Elemwise{Composite{[mul(sub(i0, sqr(i1)), i2, i3, i4)]}}[(0, 1)].0, W_rec_copy.T_replace)
        640000B  [(80, 1000)] i Gemm{inplace}(Dot22.0, TensorConstant{1.0}, Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}.0, R_rec_copy.T_replace, TensorConstant{1.0})
        640000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, <TensorType(float64, matrix)>, G_rec_copy, TensorConstant{1.0})
        640000B  [(80, 1000)] i Elemwise{ScalarSigmoid}[(0, 0)](Gemm{no_inplace}.0)
        640000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float64, matrix)>, TensorConstant{1.0}, Elemwise{mul,no_inplace}.0, W_rec_copy, TensorConstant{1.0})
        640000B  [(80, 1000)] c Dot22(Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}.0, G_rec_copy.T_replace)
        640000B  [(80, 1000)] c Elemwise{sub}(TensorConstant{(1, 1) of 1.0}, Elemwise{ScalarSigmoid}[(0, 0)].0)
        640000B  [(80, 1000)] c Elemwise{Composite{[mul(sub(i0, i1), i1, i2, i3)]}}(TensorConstant{(1, 1) of 1.0}, Elemwise{ScalarSigmoid}[(0, 0)].0, Dot22.0, <TensorType(float64, matrix)>)
        640000B  [(1000, 80)] v InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0)
        640000B  [(80, 1000)] c Elemwise{mul,no_inplace}(Elemwise{ScalarSigmoid}[(0, 0)].0, <TensorType(float64, matrix)>)
        640000B  [(80, 1000)] c Elemwise{Composite{[mul(i0, add(mul(i1, i2, i3), neg(mul(i4, i2, i3))), i5)]}}(Elemwise{sub}.0, Elemwise{Tanh}[(0, 0)].0, <TensorType(float64, matrix)>, <TensorType(float64, col)>, <TensorType(float64, matrix)>, Elemwise{ScalarSigmoid}[(0, 0)].0)
        640000B  [(80, 1000)] i Elemwise{Tanh}[(0, 0)](Gemm{no_inplace}.0)
   ... (remaining 0 Apply account for    0B/34240000B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

