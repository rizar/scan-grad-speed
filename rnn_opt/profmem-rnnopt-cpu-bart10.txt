/u/bahdanau/Dist/theano/theano/gof/vm.py:719: UserWarning: CVM does not support memory profile, using Stack VM.
  'CVM does not support memory profile, using Stack VM.')
/u/bahdanau/Dist/theano/theano/scan_module/scan_perform_ext.py:117: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Function profiling
==================
  Message: grad1
  Time in 1 calls to Function.__call__: 3.216671e+00s
  Time in Function.fn.__call__: 3.216625e+00s (99.999%)
  Time in thunks: 3.214713e+00s (99.939%)
  Total compile time: 1.610084e+00s
    Number of Apply nodes: 33
    Theano Optimizer time: 1.497732e+00s
       Theano validate time: 1.610661e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 9.725595e-02s

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  98.9%    98.9%       3.180s       1.59e+00s     Py       2       2   theano.scan_module.scan_op.Scan
   0.6%    99.5%       0.020s       3.39e-03s     C        6       6   theano.tensor.basic.Alloc
   0.4%    99.9%       0.012s       2.38e-03s     C        5       5   theano.tensor.elemwise.Elemwise
   0.1%   100.0%       0.003s       1.36e-03s     C        2       2   theano.tensor.subtensor.IncSubtensor
   0.0%   100.0%       0.000s       1.34e-05s     C        5       5   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.000s       8.28e-06s     C        7       7   theano.compile.ops.Shape_i
   0.0%   100.0%       0.000s       9.42e-06s     C        2       2   theano.tensor.elemwise.DimShuffle
   0.0%   100.0%       0.000s       8.58e-06s     C        2       2   theano.compile.ops.Rebroadcast
   0.0%   100.0%       0.000s       6.44e-06s     C        2       2   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  75.4%    75.4%       2.425s       2.43e+00s     Py       1        1   forall_inplace,cpu,grad_of_fpass}
  23.5%    98.9%       0.754s       7.54e-01s     Py       1        1   forall_inplace,cpu,fpass}
   0.6%    99.5%       0.020s       3.39e-03s     C        6        6   Alloc
   0.4%    99.9%       0.012s       1.19e-02s     C        1        1   Elemwise{Composite{[sub(i0, sqr(i1))]}}
   0.1%   100.0%       0.003s       2.64e-03s     C        1        1   IncSubtensor{InplaceInc;int64::}
   0.0%   100.0%       0.000s       7.01e-05s     C        1        1   IncSubtensor{InplaceInc;int64}
   0.0%   100.0%       0.000s       1.44e-05s     C        2        2   Subtensor{int64:int64:int64}
   0.0%   100.0%       0.000s       8.74e-06s     C        3        3   Shape_i{1}
   0.0%   100.0%       0.000s       9.42e-06s     C        2        2   Shape_i{0}
   0.0%   100.0%       0.000s       1.72e-05s     C        1        1   Subtensor{::int64}
   0.0%   100.0%       0.000s       8.58e-06s     C        2        2   Rebroadcast{0}
   0.0%   100.0%       0.000s       6.44e-06s     C        2        2   Shape_i{2}
   0.0%   100.0%       0.000s       6.44e-06s     C        2        2   ScalarFromTensor
   0.0%   100.0%       0.000s       1.12e-05s     C        1        1   Subtensor{int64:int64:int8}
   0.0%   100.0%       0.000s       1.10e-05s     C        1        1   InplaceDimShuffle{1,0}
   0.0%   100.0%       0.000s       1.00e-05s     C        1        1   Subtensor{int64}
   0.0%   100.0%       0.000s       8.11e-06s     C        1        1   Elemwise{Composite{[Switch(LT(i0, i1), i0, i1)]}}
   0.0%   100.0%       0.000s       7.87e-06s     C        1        1   InplaceDimShuffle{0,2,1}
   0.0%   100.0%       0.000s       7.15e-06s     C        1        1   Elemwise{le,no_inplace}
   0.0%   100.0%       0.000s       5.96e-06s     C        1        1   Elemwise{Composite{[Switch(i0, i1, minimum(i2, i3))]}}[(0, 3)]
   ... (remaining 1 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  75.4%    75.4%       2.425s       2.43e+00s      1    31                     forall_inplace,cpu,grad_of_fpass}(TensorConstant{50}, InplaceDimShuffle{0,2,1}.0, Elemwise{Composite{[sub(i0, sqr(i1))]}}.0, Subtensor{::int64}.0, Rebroadcast{0}.0, InplaceDimShuffle{1,0}.0)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 1000, 80), strides=(-320000, 4, 4000) 
    input 2: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 3: dtype=float32, shape=(51, 80, 1000), strides=(-320000, 4000, 4) 
    input 4: dtype=float32, shape=(1, 1000, 1000), strides=c 
    input 5: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=(-320000, 4000, 4) 
    output 1: dtype=float32, shape=(1, 1000, 1000), strides=c 
  23.5%    98.9%       0.754s       7.54e-01s      1    25                     forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, <TensorType(float32, matrix)>)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.4%    99.3%       0.012s       1.19e-02s      1    29                     Elemwise{Composite{[sub(i0, sqr(i1))]}}(TensorConstant{(1, 1, 1) of 1.0}, Subtensor{int64:int64:int64}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.3%    99.5%       0.008s       8.07e-03s      1    18                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{50}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.2%    99.8%       0.008s       7.91e-03s      1    19                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.1%    99.9%       0.003s       2.64e-03s      1    24                     IncSubtensor{InplaceInc;int64::}(Alloc.0, IncSubtensor{InplaceInc;int64}.0, Constant{1})
    input 0: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.1%    99.9%       0.002s       2.11e-03s      1     8                     Alloc(TensorConstant{0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.1%   100.0%       0.002s       2.01e-03s      1     6                     Alloc(TensorConstant{0.0}, TensorConstant{1}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 1000, 1000), strides=c 
   0.0%   100.0%       0.000s       1.80e-04s      1     7                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       7.01e-05s      1    22                     IncSubtensor{InplaceInc;int64}(Alloc.0, Alloc.0, Constant{-1})
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 1: dtype=float32, shape=(80, 1000), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       4.79e-05s      1    17                     Alloc(TensorConstant{(1, 1) of 1.0}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.72e-05s      1    26                     Subtensor{::int64}(IncSubtensor{InplaceInc;int64::}.0, Constant{-1})
    input 0: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=(-320000, 4000, 4) 
   0.0%   100.0%       0.000s       1.60e-05s      1    28                     Subtensor{int64:int64:int64}(forall_inplace,cpu,fpass}.0, Constant{49}, Constant{-52}, Constant{-1})
    input 0: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
   0.0%   100.0%       0.000s       1.31e-05s      1     0                     Shape_i{1}(<TensorType(float32, matrix)>)
    input 0: dtype=float32, shape=(1000, 1000), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.0%   100.0%       0.000s       1.29e-05s      1    27                     Subtensor{int64:int64:int64}(forall_inplace,cpu,fpass}.0, Constant{50}, Constant{0}, Constant{-1})
    input 0: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
   0.0%   100.0%       0.000s       1.12e-05s      1    23                     Subtensor{int64:int64:int8}(x, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int8, shape=1, strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.10e-05s      1     2                     InplaceDimShuffle{1,0}(<TensorType(float32, matrix)>)
    input 0: dtype=float32, shape=(1000, 1000), strides=c 
    output 0: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
   0.0%   100.0%       0.000s       1.10e-05s      1     5                     Shape_i{0}(x)
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.0%   100.0%       0.000s       1.00e-05s      1    32                     Subtensor{int64}(forall_inplace,cpu,grad_of_fpass}.1, Constant{0})
    input 0: dtype=float32, shape=(1, 1000, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(1000, 1000), strides=c 
   0.0%   100.0%       0.000s       9.06e-06s      1    10                     Rebroadcast{0}(Alloc.0)
    input 0: dtype=float32, shape=(1, 1000, 1000), strides=c 
    output 0: dtype=float32, shape=(1, 1000, 1000), strides=c 
   ... (remaining 13 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 67656KB (67656KB)
    Max if linker=cvm(default): 51406KB (51719KB)
    Memory saved if views are used: 90469KB (90469KB)
    Memory saved if inplace ops are used: 67344KB (67344KB)
    Memory saved if gc is enabled: 16250KB (15937KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

      20320000B  [(51, 80, 1000), (1, 1000, 1000)] i i forall_inplace,cpu,grad_of_fpass}(TensorConstant{50}, InplaceDimShuffle{0,2,1}.0, Elemwise{Composite{[sub(i0, sqr(i1))]}}.0, Subtensor{::int64}.0, Rebroadcast{0}.0, InplaceDimShuffle{1,0}.0)
      16320000B  [(51, 80, 1000)] i forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, <TensorType(float32, matrix)>)
      16320000B  [(51, 80, 1000)] v Subtensor{::int64}(IncSubtensor{InplaceInc;int64::}.0, Constant{-1})
      16320000B  [(51, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
      16320000B  [(51, 80, 1000)] c Alloc(TensorConstant{0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
      16320000B  [(51, 80, 1000)] i IncSubtensor{InplaceInc;int64::}(Alloc.0, IncSubtensor{InplaceInc;int64}.0, Constant{1})
      16000000B  [(50, 1000, 80)] v InplaceDimShuffle{0,2,1}(Subtensor{int64:int64:int64}.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int8}(x, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})
      16000000B  [(50, 80, 1000)] i IncSubtensor{InplaceInc;int64}(Alloc.0, Alloc.0, Constant{-1})
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(forall_inplace,cpu,fpass}.0, Constant{49}, Constant{-52}, Constant{-1})
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(forall_inplace,cpu,fpass}.0, Constant{50}, Constant{0}, Constant{-1})
      16000000B  [(50, 80, 1000)] c Elemwise{Composite{[sub(i0, sqr(i1))]}}(TensorConstant{(1, 1, 1) of 1.0}, Subtensor{int64:int64:int64}.0)
      16000000B  [(50, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{50}, Shape_i{1}.0, Shape_i{2}.0)
       4000000B  [(1000, 1000)] v Subtensor{int64}(forall_inplace,cpu,grad_of_fpass}.1, Constant{0})
       4000000B  [(1, 1000, 1000)] c Alloc(TensorConstant{0.0}, TensorConstant{1}, Shape_i{0}.0, Shape_i{1}.0)
       4000000B  [(1, 1000, 1000)] v Rebroadcast{0}(Alloc.0)
       4000000B  [(1000, 1000)] v InplaceDimShuffle{1,0}(<TensorType(float32, matrix)>)
        320000B  [(1, 80, 1000)] v Rebroadcast{0}(Alloc.0)
        320000B  [(1, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
        320000B  [(80, 1000)] c Alloc(TensorConstant{(1, 1) of 1.0}, Shape_i{1}.0, Shape_i{2}.0)
   ... (remaining 13 Apply account for   97B/230880097B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( fpass )
==================
  Message: None
  Time in 1 calls of the op (for a total of 50 steps) 7.542419e-01s

  Total time spent in calling the VM 7.510529e-01s (99.577%)
  Total overhead (computing slices..) 3.189087e-03s (0.423%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  60.0%    60.0%       0.444s       8.88e-03s     C       50       1   theano.tensor.blas.Gemm
  40.0%   100.0%       0.297s       5.93e-03s     C       50       1   theano.tensor.elemwise.Elemwise
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  60.0%    60.0%       0.444s       8.88e-03s     C       50        1   Gemm{no_inplace}
  40.0%   100.0%       0.297s       5.93e-03s     C       50        1   Elemwise{Tanh}[(0, 0)]
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  60.0%    60.0%       0.444s       8.88e-03s     50     0                     Gemm{no_inplace}(x[t], TensorConstant{1.0}, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, TensorConstant{1.0})
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
  40.0%   100.0%       0.297s       5.93e-03s     50     1                     Elemwise{Tanh}[(0, 0)](Gemm{no_inplace}.0)
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 313KB (313KB)
    Max if linker=cvm(default): 313KB (313KB)
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 313KB (313KB)
    Memory saved if gc is enabled: 0KB (0KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

        320000B  [(80, 1000)] c Gemm{no_inplace}(x[t], TensorConstant{1.0}, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, TensorConstant{1.0})
        320000B  [(80, 1000)] i Elemwise{Tanh}[(0, 0)](Gemm{no_inplace}.0)
   ... (remaining 0 Apply account for    0B/640000B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( grad_of_fpass )
==================
  Message: None
  Time in 1 calls of the op (for a total of 50 steps) 2.425146e+00s

  Total time spent in calling the VM 2.392751e+00s (98.664%)
  Total overhead (computing slices..) 3.239560e-02s (1.336%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  99.5%    99.5%       2.367s       2.37e-02s     C      100       2   theano.tensor.blas.Gemm
   0.5%   100.0%       0.013s       2.52e-04s     C       50       1   theano.tensor.elemwise.Elemwise
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  99.5%    99.5%       2.367s       2.37e-02s     C      100        2   Gemm{no_inplace}
   0.5%   100.0%       0.013s       2.52e-04s     C       50        1   Elemwise{mul}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  51.4%    51.4%       1.223s       2.45e-02s     50     2                     Gemm{no_inplace}(<TensorType(float32, matrix)>, TensorConstant{1.0}, Elemwise{mul}.0, <TensorType(float32, matrix)>, TensorConstant{1.0})
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
  48.1%    99.5%       1.144s       2.29e-02s     50     1                     Gemm{no_inplace}(<TensorType(float32, matrix)>, TensorConstant{1.0}, <TensorType(float32, matrix)>, Elemwise{mul}.0, TensorConstant{1.0})
    input 0: dtype=float32, shape=(1000, 1000), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(1000, 80), strides=(4, 4000) 
    input 3: dtype=float32, shape=(80, 1000), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(1000, 1000), strides=c 
   0.5%   100.0%       0.013s       2.52e-04s     50     0                     Elemwise{mul}(<TensorType(float32, matrix)>, <TensorType(float32, matrix)>)
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    input 1: dtype=float32, shape=(80, 1000), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 4531KB (4531KB)
    Max if linker=cvm(default): 4531KB (4531KB)
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

       4000000B  [(1000, 1000)] c Gemm{no_inplace}(<TensorType(float32, matrix)>, TensorConstant{1.0}, <TensorType(float32, matrix)>, Elemwise{mul}.0, TensorConstant{1.0})
        320000B  [(80, 1000)] c Gemm{no_inplace}(<TensorType(float32, matrix)>, TensorConstant{1.0}, Elemwise{mul}.0, <TensorType(float32, matrix)>, TensorConstant{1.0})
        320000B  [(80, 1000)] c Elemwise{mul}(<TensorType(float32, matrix)>, <TensorType(float32, matrix)>)
   ... (remaining 0 Apply account for    0B/4640000B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: grad2
  Time in 1 calls to Function.__call__: 3.321991e+00s
  Time in Function.fn.__call__: 3.321944e+00s (99.999%)
  Time in thunks: 3.320314e+00s (99.950%)
  Total compile time: 1.693558e+00s
    Number of Apply nodes: 41
    Theano Optimizer time: 1.561062e+00s
       Theano validate time: 2.324653e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.196618e-01s

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  64.4%    64.4%       2.137s       1.07e+00s     Py       2       2   theano.scan_module.scan_op.Scan
  25.7%    90.1%       0.853s       8.53e-01s     C        1       1   theano.tensor.blas.Dot22
   9.0%    99.0%       0.298s       5.96e-02s     C        5       5   theano.tensor.elemwise.Elemwise
   0.6%    99.6%       0.019s       6.36e-03s     C        3       3   theano.tensor.basic.Join
   0.4%   100.0%       0.012s       1.75e-03s     C        7       7   theano.tensor.basic.Alloc
   0.0%   100.0%       0.000s       7.58e-05s     C        1       1   theano.tensor.subtensor.IncSubtensor
   0.0%   100.0%       0.000s       5.38e-06s     C        9       9   theano.compile.ops.Shape_i
   0.0%   100.0%       0.000s       1.01e-05s     C        4       4   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.000s       7.31e-06s     C        3       3   theano.tensor.elemwise.DimShuffle
   0.0%   100.0%       0.000s       5.96e-06s     C        2       2   theano.tensor.basic.Reshape
   0.0%   100.0%       0.000s       5.01e-06s     C        2       2   theano.compile.ops.Rebroadcast
   0.0%   100.0%       0.000s       4.53e-06s     C        2       2   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  41.9%    41.9%       1.390s       1.39e+00s     Py       1        1   forall_inplace,cpu,bpass}
  25.7%    67.6%       0.853s       8.53e-01s     C        1        1   Dot22
  22.5%    90.1%       0.747s       7.47e-01s     Py       1        1   forall_inplace,cpu,fpass}
   9.0%    99.0%       0.298s       2.98e-01s     C        1        1   Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)]
   0.6%    99.6%       0.019s       6.36e-03s     C        3        3   Join
   0.4%   100.0%       0.012s       1.75e-03s     C        7        7   Alloc
   0.0%   100.0%       0.000s       7.58e-05s     C        1        1   IncSubtensor{InplaceSet;:int64:}
   0.0%   100.0%       0.000s       5.30e-06s     C        4        4   Shape_i{2}
   0.0%   100.0%       0.000s       4.53e-06s     C        4        4   Shape_i{1}
   0.0%   100.0%       0.000s       8.46e-06s     C        2        2   InplaceDimShuffle{2,0,1}
   0.0%   100.0%       0.000s       7.63e-06s     C        2        2   Subtensor{int64:int64:int8}
   0.0%   100.0%       0.000s       1.29e-05s     C        1        1   Subtensor{:int64:}
   0.0%   100.0%       0.000s       1.22e-05s     C        1        1   Subtensor{int64:int64:int64}
   0.0%   100.0%       0.000s       5.96e-06s     C        2        2   Reshape{2}
   0.0%   100.0%       0.000s       5.01e-06s     C        2        2   Rebroadcast{0}
   0.0%   100.0%       0.000s       9.06e-06s     C        1        1   Shape_i{0}
   0.0%   100.0%       0.000s       4.53e-06s     C        2        2   ScalarFromTensor
   0.0%   100.0%       0.000s       5.01e-06s     C        1        1   Elemwise{Composite{[Switch(i0, i1, minimum(i2, i3))]}}[(0, 3)]
   0.0%   100.0%       0.000s       5.01e-06s     C        1        1   Elemwise{Composite{[Switch(LT(i0, i1), i0, i1)]}}
   0.0%   100.0%       0.000s       5.01e-06s     C        1        1   Elemwise{le,no_inplace}
   ... (remaining 2 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  41.9%    41.9%       1.390s       1.39e+00s      1    35                     forall_inplace,cpu,bpass}(TensorConstant{50}, Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)].0, IncSubtensor{InplaceSet;:int64:}.0, <TensorType(float32, matrix)>)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
    input 2: dtype=float32, shape=(49, 80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
  25.7%    67.6%       0.853s       8.53e-01s      1    40                     Dot22(Reshape{2}.0, InplaceDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(1000, 4000), strides=(4, 4000) 
    input 1: dtype=float32, shape=(4000, 1000), strides=c 
    output 0: dtype=float32, shape=(1000, 1000), strides=c 
  22.5%    90.1%       0.747s       7.47e-01s      1    23                     forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, TensorConstant{50}, <TensorType(float32, matrix)>)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 3: dtype=int8, shape=(), strides=c 
    input 4: dtype=float32, shape=(1000, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    output 1: dtype=float32, shape=(50, 80, 1000), strides=c 
   9.0%    99.0%       0.298s       2.98e-01s      1    34                     Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)](Subtensor{int64:int64:int64}.0)
    input 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
   0.2%    99.3%       0.008s       8.28e-03s      1    29                     Join(TensorConstant{0}, Alloc.0, Subtensor{int64:int64:int8}.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(1, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(49, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.2%    99.5%       0.008s       7.97e-03s      1    36                     Join(TensorConstant{0}, forall_inplace,cpu,bpass}.0, Alloc.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(49, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(1, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.2%    99.7%       0.006s       5.97e-03s      1     4                     Alloc(TensorConstant{0.0}, TensorConstant{49}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
   0.2%    99.9%       0.006s       5.92e-03s      1     5                     Alloc(TensorConstant{0.0}, TensorConstant{50}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.1%   100.0%       0.003s       2.84e-03s      1    30                     Join(TensorConstant{0}, Alloc.0, Subtensor{:int64:}.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(1, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(49, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.37e-04s      1    18                     Alloc(TensorConstant{(1, 1, 1) of 1.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.22e-04s      1     3                     Alloc(TensorConstant{(1, 1, 1) of 1.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       7.58e-05s      1    13                     IncSubtensor{InplaceSet;:int64:}(Alloc.0, Rebroadcast{0}.0, Constant{1})
    input 0: dtype=float32, shape=(49, 80, 1000), strides=c 
    input 1: dtype=float32, shape=(1, 80, 1000), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       3.91e-05s      1    28                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       3.41e-05s      1     6                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       3.10e-05s      1    19                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.29e-05s      1    24                     Subtensor{:int64:}(forall_inplace,cpu,fpass}.1, Constant{-1})
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.22e-05s      1    32                     Subtensor{int64:int64:int64}(Join.0, Constant{49}, Constant{-51}, Constant{-1})
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
   0.0%   100.0%       0.000s       9.06e-06s      1    37                     InplaceDimShuffle{2,0,1}(Join.0)
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(1000, 50, 80), strides=(4, 320000, 4000) 
   0.0%   100.0%       0.000s       9.06e-06s      1     2                     Shape_i{0}(x)
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    output 0: dtype=int64, shape=(), strides=c 
   0.0%   100.0%       0.000s       8.11e-06s      1    27                     Subtensor{int64:int64:int8}(forall_inplace,cpu,fpass}.0, Constant{0}, Constant{49}, Constant{1})
    input 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 1: dtype=int64, shape=8, strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    input 3: dtype=int8, shape=1, strides=c 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
   ... (remaining 21 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 98906KB (98906KB)
    Max if linker=cvm(default): 62813KB (63125KB)
    Memory saved if views are used: 140625KB (140625KB)
    Memory saved if inplace ops are used: 61875KB (61875KB)
    Memory saved if gc is enabled: 36093KB (35781KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

      32000000B  [(50, 80, 1000), (50, 80, 1000)] i c forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, TensorConstant{50}, <TensorType(float32, matrix)>)
      16000000B  [(50, 80, 1000)] c Join(TensorConstant{0}, forall_inplace,cpu,bpass}.0, Alloc.0)
      16000000B  [(50, 80, 1000)] c Join(TensorConstant{0}, Alloc.0, Subtensor{:int64:}.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(Join.0, Constant{49}, Constant{-51}, Constant{-1})
      16000000B  [(1000, 4000)] v Reshape{2}(InplaceDimShuffle{2,0,1}.0, TensorConstant{[1000 4000]})
      16000000B  [(4000, 1000)] v InplaceDimShuffle{1,0}(Reshape{2}.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int8}(x, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})
      16000000B  [(50, 80, 1000)] i Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)](Subtensor{int64:int64:int64}.0)
      16000000B  [(1000, 50, 80)] v InplaceDimShuffle{2,0,1}(Join.0)
      16000000B  [(1000, 50, 80)] v InplaceDimShuffle{2,0,1}(Join.0)
      16000000B  [(50, 80, 1000)] c Join(TensorConstant{0}, Alloc.0, Subtensor{int64:int64:int8}.0)
      16000000B  [(50, 80, 1000)] c Alloc(TensorConstant{0.0}, TensorConstant{50}, Shape_i{1}.0, Shape_i{2}.0)
      16000000B  [(1000, 4000)] v Reshape{2}(InplaceDimShuffle{2,0,1}.0, TensorConstant{[1000 4000]})
      15680000B  [(49, 80, 1000)] v Subtensor{:int64:}(forall_inplace,cpu,fpass}.1, Constant{-1})
      15680000B  [(49, 80, 1000)] v Subtensor{int64:int64:int8}(forall_inplace,cpu,fpass}.0, Constant{0}, Constant{49}, Constant{1})
      15680000B  [(49, 80, 1000)] i IncSubtensor{InplaceSet;:int64:}(Alloc.0, Rebroadcast{0}.0, Constant{1})
      15680000B  [(49, 80, 1000)] i forall_inplace,cpu,bpass}(TensorConstant{50}, Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)].0, IncSubtensor{InplaceSet;:int64:}.0, <TensorType(float32, matrix)>)
      15680000B  [(49, 80, 1000)] c Alloc(TensorConstant{0.0}, TensorConstant{49}, Shape_i{1}.0, Shape_i{2}.0)
       4000000B  [(1000, 1000)] c Dot22(Reshape{2}.0, InplaceDimShuffle{1,0}.0)
        320000B  [(1, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 1.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
   ... (remaining 21 Apply account for 1920113B/308640113B ((0.62%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( fpass )
==================
  Message: None
  Time in 1 calls of the op (for a total of 50 steps) 7.472811e-01s

  Total time spent in calling the VM 7.263367e-01s (97.197%)
  Total overhead (computing slices..) 2.094436e-02s (2.803%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  60.9%    60.9%       0.438s       8.75e-03s     C       50       1   theano.tensor.blas.Gemm
  39.1%   100.0%       0.281s       5.61e-03s     C       50       1   theano.tensor.elemwise.Elemwise
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  60.9%    60.9%       0.438s       8.75e-03s     C       50        1   Gemm{no_inplace}
  39.1%   100.0%       0.281s       5.61e-03s     C       50        1   Elemwise{tanh,no_inplace}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  60.9%    60.9%       0.438s       8.75e-03s     50     0                     Gemm{no_inplace}(x[t], TensorConstant{1.0}, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, TensorConstant{1.0})
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=c 
    input 4: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
  39.1%   100.0%       0.281s       5.61e-03s     50     1                     Elemwise{tanh,no_inplace}(Gemm{no_inplace}.0)
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 625KB (625KB)
    Max if linker=cvm(default): 625KB (625KB)
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

        320000B  [(80, 1000)] c Elemwise{tanh,no_inplace}(Gemm{no_inplace}.0)
        320000B  [(80, 1000)] c Gemm{no_inplace}(x[t], TensorConstant{1.0}, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, TensorConstant{1.0})
   ... (remaining 0 Apply account for    0B/640000B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.


Scan Op profiling ( bpass )
==================
  Message: None
  Time in 1 calls of the op (for a total of 50 steps) 1.389857e+00s

  Total time spent in calling the VM 1.380875e+00s (99.354%)
  Total overhead (computing slices..) 8.981943e-03s (0.646%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  98.5%    98.5%       1.352s       2.70e-02s     C       50       1   theano.tensor.blas.Dot22
   1.5%   100.0%       0.021s       4.24e-04s     C       50       1   theano.tensor.elemwise.Elemwise
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  98.5%    98.5%       1.352s       2.70e-02s     C       50        1   Dot22
   1.5%   100.0%       0.021s       4.24e-04s     C       50        1   Elemwise{true_div,no_inplace}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  98.5%    98.5%       1.352s       2.70e-02s     50     1                     Dot22(Elemwise{true_div,no_inplace}.0, <TensorType(float32, matrix)>)
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    input 1: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
   1.5%   100.0%       0.021s       4.24e-04s     50     0                     Elemwise{true_div,no_inplace}(<TensorType(float32, matrix)>, <TensorType(float32, matrix)>)
    input 0: dtype=float32, shape=(80, 1000), strides=c 
    input 1: dtype=float32, shape=(80, 1000), strides=c 
    output 0: dtype=float32, shape=(80, 1000), strides=c 
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 625KB (625KB)
    Max if linker=cvm(default): 625KB (625KB)
    Memory saved if views are used: 0KB (0KB)
    Memory saved if inplace ops are used: 0KB (0KB)
    Memory saved if gc is enabled: 0KB (0KB)

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

        320000B  [(80, 1000)] c Dot22(Elemwise{true_div,no_inplace}.0, <TensorType(float32, matrix)>)
        320000B  [(80, 1000)] c Elemwise{true_div,no_inplace}(<TensorType(float32, matrix)>, <TensorType(float32, matrix)>)
   ... (remaining 0 Apply account for    0B/640000B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Function profiling
==================
  Message: Sum of all(2) printed profiles at exit excluding Scan op profile.
  Time in 2 calls to Function.__call__: 6.538662e+00s
  Time in Function.fn.__call__: 6.538569e+00s (99.999%)
  Time in thunks: 6.535027e+00s (99.944%)
  Total compile time: 3.303642e+00s
    Number of Apply nodes: 74
    Theano Optimizer time: 3.058794e+00s
       Theano validate time: 3.935313e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.169178e-01s

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  81.4%    81.4%       5.317s       1.33e+00s     Py       4       4   theano.scan_module.scan_op.Scan
  13.1%    94.4%       0.853s       8.53e-01s     C        1       1   theano.tensor.blas.Dot22
   4.7%    99.2%       0.310s       3.10e-02s     C       10      10   theano.tensor.elemwise.Elemwise
   0.5%    99.7%       0.033s       2.51e-03s     C       13      13   theano.tensor.basic.Alloc
   0.3%   100.0%       0.019s       6.36e-03s     C        3       3   theano.tensor.basic.Join
   0.0%   100.0%       0.003s       9.29e-04s     C        3       3   theano.tensor.subtensor.IncSubtensor
   0.0%   100.0%       0.000s       1.19e-05s     C        9       9   theano.tensor.subtensor.Subtensor
   0.0%   100.0%       0.000s       6.65e-06s     C       16      16   theano.compile.ops.Shape_i
   0.0%   100.0%       0.000s       8.15e-06s     C        5       5   theano.tensor.elemwise.DimShuffle
   0.0%   100.0%       0.000s       6.79e-06s     C        4       4   theano.compile.ops.Rebroadcast
   0.0%   100.0%       0.000s       5.48e-06s     C        4       4   theano.tensor.basic.ScalarFromTensor
   0.0%   100.0%       0.000s       5.96e-06s     C        2       2   theano.tensor.basic.Reshape
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  37.1%    37.1%       2.425s       2.43e+00s     Py       1        1   forall_inplace,cpu,grad_of_fpass}
  21.3%    58.4%       1.390s       1.39e+00s     Py       1        1   forall_inplace,cpu,bpass}
  13.1%    71.4%       0.853s       8.53e-01s     C        1        1   Dot22
  11.5%    83.0%       0.754s       7.54e-01s     Py       1        1   forall_inplace,cpu,fpass}
  11.4%    94.4%       0.747s       7.47e-01s     Py       1        1   forall_inplace,cpu,fpass}
   4.6%    99.0%       0.298s       2.98e-01s     C        1        1   Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)]
   0.5%    99.5%       0.033s       2.51e-03s     C       13       13   Alloc
   0.3%    99.8%       0.019s       6.36e-03s     C        3        3   Join
   0.2%   100.0%       0.012s       1.19e-02s     C        1        1   Elemwise{Composite{[sub(i0, sqr(i1))]}}
   0.0%   100.0%       0.003s       2.64e-03s     C        1        1   IncSubtensor{InplaceInc;int64::}
   0.0%   100.0%       0.000s       7.58e-05s     C        1        1   IncSubtensor{InplaceSet;:int64:}
   0.0%   100.0%       0.000s       7.01e-05s     C        1        1   IncSubtensor{InplaceInc;int64}
   0.0%   100.0%       0.000s       6.34e-06s     C        7        7   Shape_i{1}
   0.0%   100.0%       0.000s       1.37e-05s     C        3        3   Subtensor{int64:int64:int64}
   0.0%   100.0%       0.000s       5.68e-06s     C        6        6   Shape_i{2}
   0.0%   100.0%       0.000s       9.30e-06s     C        3        3   Shape_i{0}
   0.0%   100.0%       0.000s       6.79e-06s     C        4        4   Rebroadcast{0}
   0.0%   100.0%       0.000s       8.82e-06s     C        3        3   Subtensor{int64:int64:int8}
   0.0%   100.0%       0.000s       5.48e-06s     C        4        4   ScalarFromTensor
   0.0%   100.0%       0.000s       1.72e-05s     C        1        1   Subtensor{::int64}
   ... (remaining 10 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  37.1%    37.1%       2.425s       2.43e+00s      1    31                     forall_inplace,cpu,grad_of_fpass}(TensorConstant{50}, InplaceDimShuffle{0,2,1}.0, Elemwise{Composite{[sub(i0, sqr(i1))]}}.0, Subtensor{::int64}.0, Rebroadcast{0}.0, InplaceDimShuffle{1,0}.0)
    input 0: dtype=int64, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 1000, 80), strides=(-320000, 4, 4000) 
    input 2: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 3: dtype=float32, shape=(51, 80, 1000), strides=(-320000, 4000, 4) 
    input 4: dtype=float32, shape=(1, 1000, 1000), strides=c 
    input 5: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=(-320000, 4000, 4) 
    output 1: dtype=float32, shape=(1, 1000, 1000), strides=c 
  21.3%    58.4%       1.390s       1.39e+00s      1    35                     forall_inplace,cpu,bpass}(TensorConstant{50}, Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)].0, IncSubtensor{InplaceSet;:int64:}.0, <TensorType(float32, matrix)>)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
    input 2: dtype=float32, shape=(49, 80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=(4, 4000) 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
  13.1%    71.4%       0.853s       8.53e-01s      1    40                     Dot22(Reshape{2}.0, InplaceDimShuffle{1,0}.0)
    input 0: dtype=float32, shape=(1000, 4000), strides=(4, 4000) 
    input 1: dtype=float32, shape=(4000, 1000), strides=c 
    output 0: dtype=float32, shape=(1000, 1000), strides=c 
  11.5%    83.0%       0.754s       7.54e-01s      1    25                     forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, <TensorType(float32, matrix)>)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 3: dtype=float32, shape=(1000, 1000), strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
  11.4%    94.4%       0.747s       7.47e-01s      1    23                     forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, TensorConstant{50}, <TensorType(float32, matrix)>)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 3: dtype=int8, shape=(), strides=c 
    input 4: dtype=float32, shape=(1000, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
    output 1: dtype=float32, shape=(50, 80, 1000), strides=c 
   4.6%    99.0%       0.298s       2.98e-01s      1    34                     Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)](Subtensor{int64:int64:int64}.0)
    input 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
   0.2%    99.2%       0.012s       1.19e-02s      1    29                     Elemwise{Composite{[sub(i0, sqr(i1))]}}(TensorConstant{(1, 1, 1) of 1.0}, Subtensor{int64:int64:int64}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=(-320000, 4000, 4) 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.1%    99.3%       0.008s       8.28e-03s      1    29                     Join(TensorConstant{0}, Alloc.0, Subtensor{int64:int64:int8}.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(1, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(49, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.1%    99.4%       0.008s       8.07e-03s      1    18                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{50}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.1%    99.5%       0.008s       7.97e-03s      1    36                     Join(TensorConstant{0}, forall_inplace,cpu,bpass}.0, Alloc.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(49, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(1, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.1%    99.7%       0.008s       7.91e-03s      1    19                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.1%    99.7%       0.006s       5.97e-03s      1     4                     Alloc(TensorConstant{0.0}, TensorConstant{49}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(49, 80, 1000), strides=c 
   0.1%    99.8%       0.006s       5.92e-03s      1     5                     Alloc(TensorConstant{0.0}, TensorConstant{50}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.0%    99.9%       0.003s       2.84e-03s      1    30                     Join(TensorConstant{0}, Alloc.0, Subtensor{:int64:}.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(1, 80, 1000), strides=c 
    input 2: dtype=float32, shape=(49, 80, 1000), strides=c 
    output 0: dtype=float32, shape=(50, 80, 1000), strides=c 
   0.0%    99.9%       0.003s       2.64e-03s      1    24                     IncSubtensor{InplaceInc;int64::}(Alloc.0, IncSubtensor{InplaceInc;int64}.0, Constant{1})
    input 0: dtype=float32, shape=(51, 80, 1000), strides=c 
    input 1: dtype=float32, shape=(50, 80, 1000), strides=c 
    input 2: dtype=int64, shape=8, strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.0%   100.0%       0.002s       2.11e-03s      1     8                     Alloc(TensorConstant{0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(51, 80, 1000), strides=c 
   0.0%   100.0%       0.002s       2.01e-03s      1     6                     Alloc(TensorConstant{0.0}, TensorConstant{1}, Shape_i{0}.0, Shape_i{1}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 1000, 1000), strides=c 
   0.0%   100.0%       0.000s       1.80e-04s      1     7                     Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.37e-04s      1    18                     Alloc(TensorConstant{(1, 1, 1) of 1.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   0.0%   100.0%       0.000s       1.22e-04s      1     3                     Alloc(TensorConstant{(1, 1, 1) of 1.0}, TensorConstant{1}, Shape_i{1}.0, Shape_i{2}.0)
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=int64, shape=(), strides=c 
    input 2: dtype=int64, shape=(), strides=c 
    input 3: dtype=int64, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 80, 1000), strides=c 
   ... (remaining 54 Apply instances account for 0.01%(0.00s) of the runtime)

Memory Profile (the max between all functions in that profile)
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max if no gc (allow_gc=False): 98906KB (98906KB)
    Max if linker=cvm(default): 62813KB (63125KB)
    Memory saved if views are used: 140625KB (140625KB)
    Memory saved if inplace ops are used: 67344KB (67344KB)
    Memory saved if gc is enabled: 36093KB (35781KB)

    This list is based on all functions in the profile
    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

      32000000B  [(50, 80, 1000), (50, 80, 1000)] i c forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, TensorConstant{50}, <TensorType(float32, matrix)>)
      20320000B  [(51, 80, 1000), (1, 1000, 1000)] i i forall_inplace,cpu,grad_of_fpass}(TensorConstant{50}, InplaceDimShuffle{0,2,1}.0, Elemwise{Composite{[sub(i0, sqr(i1))]}}.0, Subtensor{::int64}.0, Rebroadcast{0}.0, InplaceDimShuffle{1,0}.0)
      16320000B  [(51, 80, 1000)] c Alloc(TensorConstant{(1, 1, 1) of 0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
      16320000B  [(51, 80, 1000)] v Subtensor{::int64}(IncSubtensor{InplaceInc;int64::}.0, Constant{-1})
      16320000B  [(51, 80, 1000)] c Alloc(TensorConstant{0.0}, TensorConstant{51}, Shape_i{1}.0, Shape_i{2}.0)
      16320000B  [(51, 80, 1000)] i IncSubtensor{InplaceInc;int64::}(Alloc.0, IncSubtensor{InplaceInc;int64}.0, Constant{1})
      16320000B  [(51, 80, 1000)] i forall_inplace,cpu,fpass}(TensorConstant{50}, Subtensor{int64:int64:int8}.0, Alloc.0, <TensorType(float32, matrix)>)
      16000000B  [(50, 80, 1000)] i Elemwise{Composite{[sqr(cosh(i0))]}}[(0, 0)](Subtensor{int64:int64:int64}.0)
      16000000B  [(50, 1000, 80)] v InplaceDimShuffle{0,2,1}(Subtensor{int64:int64:int64}.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(Join.0, Constant{49}, Constant{-51}, Constant{-1})
      16000000B  [(1000, 4000)] v Reshape{2}(InplaceDimShuffle{2,0,1}.0, TensorConstant{[1000 4000]})
      16000000B  [(4000, 1000)] v InplaceDimShuffle{1,0}(Reshape{2}.0)
      16000000B  [(50, 80, 1000)] c Join(TensorConstant{0}, Alloc.0, Subtensor{:int64:}.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(forall_inplace,cpu,fpass}.0, Constant{49}, Constant{-52}, Constant{-1})
      16000000B  [(1000, 50, 80)] v InplaceDimShuffle{2,0,1}(Join.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int8}(x, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})
      16000000B  [(50, 80, 1000)] i IncSubtensor{InplaceInc;int64}(Alloc.0, Alloc.0, Constant{-1})
      16000000B  [(50, 80, 1000)] c Elemwise{Composite{[sub(i0, sqr(i1))]}}(TensorConstant{(1, 1, 1) of 1.0}, Subtensor{int64:int64:int64}.0)
      16000000B  [(50, 80, 1000)] v Subtensor{int64:int64:int64}(forall_inplace,cpu,fpass}.0, Constant{50}, Constant{0}, Constant{-1})
      16000000B  [(50, 80, 1000)] c Join(TensorConstant{0}, Alloc.0, Subtensor{int64:int64:int8}.0)
   ... (remaining 54 Apply account for 197600210B/539520210B ((36.63%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

